{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miglesia/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "trade_bra = pd.read_csv('data/econ_data/trade_bra.csv', dtype = {'HS ID 4': str}).dropna(subset = ['Exports', 'Imports'])\n",
    "wages_bra = pd.read_csv('data/econ_data/wages_bra.csv').dropna(subset = ['Total Jobs', 'Total Yearly Wages'])\n",
    "trade_chi = pd.read_csv('data/econ_data/trade_chi.csv', dtype = {'HS ID 4': str}).dropna(subset = ['Exports', 'Imports'])\n",
    "prod_chi = pd.read_csv('data/econ_data/wages_output_chi.csv')#.dropna(subset = ['Total Jobs', 'Total Yearly Wages'])\n",
    "# prod_chi = pd.read_csv('data/econ_data/sources/production_USD.csv')#.dropna(subset = ['Total Jobs', 'Total Yearly Wages'])\n",
    "# wages_bra_raw = wages_bra_raw.sample(frac = .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_small_cats(df, level, cat = 'BRA ID MIR', col = 'Total Yearly Wages'):\n",
    "    grouped = df.groupby(cat).sum().sort_values(by = col, ascending = False).dropna()\n",
    "    cumsum = grouped.div(grouped.sum()).cumsum()\n",
    "    label_sel = cumsum.loc[cumsum[col] < level].index.values\n",
    "    df_ = df.loc[df[cat].isin(label_sel)]\n",
    "    return df_\n",
    "\n",
    "df1 = filter_small_cats(wages_bra, .98, 'BRA ID MIR')\n",
    "df2 = filter_small_cats(df1, .98, 'CNAE ID C')\n",
    "df3 = filter_small_cats(df2, .98, 'CBO ID F')\n",
    "\n",
    "# for df in [df1, df2, df3]:\n",
    "#     print np.true_divide(len(df),len(wages_bra))\n",
    "#     print df['Total Yearly Wages'].sum()/wages_bra['Total Yearly Wages'].sum()\n",
    "\n",
    "wages_bra = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wages_bra['Total Yearly Wages'].sum()/x['Total Yearly Wages'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info of aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load info\n",
    "path = 'data/disagg_struct_refs/formatted/'\n",
    "\n",
    "CIIU = pd.read_csv(path+'CIIU_full.csv')\n",
    "CBO = pd.read_csv(path+'CBO_full.csv').astype(str)\n",
    "CNAE = pd.read_csv(path+'CNAE_full.csv').astype(str)\n",
    "HS = pd.read_csv(path+'hs_full.csv').astype(str)\n",
    "\n",
    "#Choose columns\n",
    "CIIU_agg_ref = CIIU[['CIIU ID 4', 'CIIU ID 2', 'CIIU ID 1', 'CIIU ID 0']]\n",
    "CNAE_agg_ref = CNAE[['CNAE ID C', 'CNAE ID G', 'CNAE ID D', 'CNAE ID S','CNAE ID 0']]\n",
    "CBO_agg_ref = CBO[['CBO ID F', 'CBO ID SG', 'CBO ID SGP', 'CBO ID GG', 'CBO ID 0']]\n",
    "HS_agg_ref = HS[['HS ID 4', 'HS ID 2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wages_bra' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-875a7df30a48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Geographic info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgeo_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwages_bra\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BRA ID MIR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrade_bra\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BRA ID MIR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mGEO_bra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgid\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mgid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgeo_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'BRA ID ST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BRA ID MER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BRA ID MIR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# GEO_bra.to_csv('data/Classifications/GEO_bra.csv', index = False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wages_bra' is not defined"
     ]
    }
   ],
   "source": [
    "#Geographic info\n",
    "\n",
    "geo_ids = pd.concat([wages_bra['BRA ID MIR'], trade_bra['BRA ID MIR']]).unique()\n",
    "GEO_bra = pd.DataFrame([[gid[:3],gid[:5],gid]  for gid in geo_ids], columns = ['BRA ID ST', 'BRA ID MER', 'BRA ID MIR'])\n",
    "# GEO_bra.to_csv('data/Classifications/GEO_bra.csv', index = False)\n",
    "\n",
    "GEO_chi = pd.read_csv(path+'GEO_chi.csv')[['reg_id', 'prov_id', 'comuna_id']]\n",
    "GEO_chi = GEO_chi.rename({'reg_id':'CHI ID REG', 'prov_id':'CHI ID PRO', 'comuna_id':'CHI ID COM'},axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregate across 4 years\n",
    "\n",
    "trade_bra_4y = trade_bra.groupby([u'BRA ID MIR', u'HS ID 4']).sum().reset_index()\n",
    "wages_bra_4y = wages_bra.groupby([ u'BRA ID MIR',u'CNAE ID C', u'CBO ID F']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Geo_lev = 'MIR'\n",
    "\n",
    "trade_bra_4y_gl = trade_bra_4y.merge(GEO_bra).groupby(['BRA ID '+Geo_lev, 'HS ID 4']).sum().reset_index()\n",
    "wages_bra_4y_gl = wages_bra_4y.merge(GEO_bra).groupby(['BRA ID '+Geo_lev, 'CNAE ID C', 'CBO ID F']).sum().reset_index()\n",
    "\n",
    "nodes_df_list = []\n",
    "df_list =[]\n",
    "confs = [('0', '0', '2', '2'),\n",
    "         ('0', 'S', '2', '2'),\n",
    "         ('GG', '0', '2', '2'),\n",
    "         ('GG', 'S', '2', '2'),\n",
    "        ('0', 'G', '4', '2')]\n",
    "name_list = []\n",
    "for conf in confs:\n",
    "    (CBO_lev, CNAE_lev, HS_Exp, HS_Imp) = conf\n",
    "    \n",
    "    col_name = 'corr_'+Geo_lev+'_'+'_'.join([c for c in conf])\n",
    "    name_list += [col_name]\n",
    "\n",
    "    # Bring 3 datasets and give them node_names. Prepare for correlation\n",
    "\n",
    "    x = wages_bra_4y_gl.merge(CBO_agg_ref).groupby(['BRA ID '+Geo_lev, 'CNAE ID C', 'CBO ID '+CBO_lev]).sum()[['Total Yearly Wages']].reset_index()\n",
    "    print '%.2E' % Decimal(x['Total Yearly Wages'].sum())\n",
    "    x = x.merge(CNAE_agg_ref).groupby(['BRA ID '+Geo_lev, 'CNAE ID '+CNAE_lev, 'CBO ID '+CBO_lev]).sum()[['Total Yearly Wages']].reset_index()\n",
    "    x_ = x\n",
    "\n",
    "    #     use label for naming nodes\n",
    "    x_ = x.merge(CNAE[['CNAE ID '+CNAE_lev,'CNAE label '+CNAE_lev]].drop_duplicates())\n",
    "    x_ = x_.merge(CBO[['CBO ID '+CBO_lev, 'CBO label '+CBO_lev]].drop_duplicates())\n",
    "    x_['node_name'] = 'Wages in ' + x_[[col for col in x_.columns if 'label' in col]].apply(' of '.join, axis=1)\n",
    "    # use ID for naming nodes\n",
    "    #     x_['node_name'] = 'Wages in ' + x_[[col for col in x_.columns if 'label' in col and 'BRA' not in col]].apply(' of '.join, axis=1)\n",
    "\n",
    "    x_ = x_.drop([col for col in x_.columns if 'label' in col], axis = 1)\n",
    "    \n",
    "    \n",
    "    #node size\n",
    "    x_ = x_.merge(x.groupby(['CNAE ID '+CNAE_lev,'CBO ID '+CBO_lev]).sum().reset_index().rename({'Total Yearly Wages': 'node_value_USD'}, axis = 1))\n",
    "    x = x_\n",
    "\n",
    "    if CNAE_lev != '0':\n",
    "        x = x.merge(CNAE[list(set(['CNAE ID '+CNAE_lev,'CNAE ID S']))].drop_duplicates())\n",
    "    if CBO_lev != '0':\n",
    "        x = x.merge(CBO[list(set(['CBO ID '+CBO_lev,'CBO ID GG']))].drop_duplicates())\n",
    "    x = x.rename({'CNAE ID S': 'att_A', 'CBO ID GG': 'att_B', 'Total Yearly Wages': 'value_USD'}, axis = 1)\n",
    "    print '%.2E' % Decimal(x['value_USD'].sum())\n",
    "\n",
    "\n",
    "    # Trade in HS2\n",
    "\n",
    "    ### Exports\n",
    "    y = trade_bra_4y_gl.merge(HS[['HS ID 4', 'HS ID 2']].drop_duplicates()).groupby(['BRA ID '+Geo_lev, 'HS ID '+HS_Exp]).sum().reset_index()\n",
    "    print '%.2E' % Decimal(y['Exports'].sum())\n",
    "    y = y.drop([col for col in y.columns if 'Imp' in col], axis = 1)\n",
    "    y_ = y.merge(HS[['HS ID '+HS_Exp,'HS label '+HS_Exp]].drop_duplicates())\n",
    "    y_['node_name'] = 'Exports of ' + y_[[col for col in y_.columns if 'label' in col]].apply(' of '.join, axis=1)\n",
    "    y_ = y_.drop([col for col in y_.columns if 'label' in col], axis = 1)\n",
    "\n",
    "    \n",
    "    node_val = y.groupby(['HS ID '+HS_Exp]).sum()[['Exports']].rename({'Exports': 'node_value_USD'}, axis = 1).reset_index()\n",
    "    y_ = y_.merge(node_val, on = 'HS ID '+HS_Exp)\n",
    "\n",
    "    y = y_\n",
    "    y = y.rename({'HS ID '+HS_Exp: 'att_A', 'Exports': 'value_USD'}, axis = 1)\n",
    "    y['att_B'] = y['att_A'] = 'Exports'\n",
    "    \n",
    "    print '%.2E' % Decimal(y['value_USD'].sum())\n",
    "    \n",
    "    ### Imports\n",
    "    z = trade_bra_4y_gl.merge(HS[['HS ID 4', 'HS ID 2']].drop_duplicates()).groupby(['BRA ID '+Geo_lev, 'HS ID '+HS_Imp]).sum().reset_index()\n",
    "    print '%.2E' % Decimal(z['Imports'].sum())\n",
    "    z = z.drop([col for col in z.columns if 'Exp' in col], axis = 1)\n",
    "    z_ = z.merge(HS[['HS ID '+HS_Imp,'HS label '+HS_Imp]].drop_duplicates())\n",
    "\n",
    "    z_['node_name'] = 'Imports of ' + z_[[col for col in z_.columns if 'label' in col]].apply(' of '.join, axis=1)\n",
    "    z_ = z_.drop([col for col in z_.columns if 'label' in col], axis = 1)\n",
    "    \n",
    "    node_val = z_.groupby(['HS ID '+HS_Imp]).sum()[['Imports']].rename({'Imports': 'node_value_USD'}, axis = 1).reset_index()\n",
    "    z_ = z_.merge(node_val, on = 'HS ID '+HS_Imp)\n",
    " \n",
    "    z = z_\n",
    "    z = z.rename({'HS ID '+HS_Imp: 'att_A', 'Imports': 'value_USD'}, axis = 1)\n",
    "    z['att_B'] = z['att_A'] = 'Imports'\n",
    "    print '%.2E' % Decimal(z['value_USD'].sum())\n",
    "\n",
    "    ###\n",
    "\n",
    "    concat_data = pd.concat([x, y, z])\n",
    "\n",
    "\n",
    "\n",
    "    #Data for correlation\n",
    "    df = concat_data.groupby(['BRA ID '+Geo_lev, 'node_name']).sum()[['value_USD']]\n",
    "    \n",
    "    print df.sum()\n",
    "    \n",
    "    #Drop small categories\n",
    "    th = .99\n",
    "    cat_size = df.reset_index().groupby('node_name').sum()[['value_USD']].sort_values(by = 'value_USD', ascending = False)\n",
    "    cat_cumsum = (cat_size/cat_size.sum()).cumsum()\n",
    "    l = list(cat_cumsum.loc[cat_cumsum['value_USD'] < th].index)   \n",
    "    df = df.reset_index().loc[df.reset_index().node_name.isin(l)].set_index(['BRA ID '+Geo_lev, 'node_name'])\n",
    "    \n",
    "    #Save nodes info\n",
    "    nodes_df = concat_data.groupby('node_name').first()[['att_A', 'att_B', 'node_value_USD']]\n",
    "    nodes_df = nodes_df.reset_index().loc[nodes_df.reset_index().node_name.isin(l)].set_index(['node_name'])\n",
    "    nodes_df_list += [nodes_df]\n",
    "    \n",
    "    # (cat_size/cat_size.sum()).cumsum().reset_index().plot(y = 'value_USD')\n",
    "    # plt.show()\n",
    "    \n",
    "    print df.sum()\n",
    "\n",
    "    \n",
    "    df = df.unstack()\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "\n",
    "\n",
    "    # correlations\n",
    "    robust_th = 5\n",
    "    corr_geolevel_yr = np.log10(df).corr(min_periods = robust_th)\n",
    "    del corr_geolevel_yr.index.name\n",
    "    del corr_geolevel_yr.columns.name\n",
    "\n",
    "    df_list += [pd.DataFrame(corr_geolevel_yr.stack(), columns = [col_name])]\n",
    "\n",
    "    print 'done conf '+ str(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Data for correlation\n",
    "# df = pd.concat([x, y, z]).groupby(['BRA ID '+Geo_lev, 'node_name']).sum()[['value_USD']]\n",
    "\n",
    "# print df.sum()\n",
    "\n",
    "# #Drop small categories\n",
    "# th = .99\n",
    "# cat_size = df.reset_index().groupby('node_name').sum()[['value_USD']].sort_values(by = 'value_USD', ascending = False)\n",
    "# cat_cumsum = (cat_size/cat_size.sum()).cumsum()\n",
    "# l = list(cat_cumsum.loc[cat_cumsum['value_USD'] < th].index)   \n",
    "# df = df.reset_index().loc[df.reset_index().node_name.isin(l)].set_index(['BRA ID '+Geo_lev, 'node_name'])\n",
    "\n",
    "# # (cat_size/cat_size.sum()).cumsum().reset_index().plot(y = 'value_USD')\n",
    "# # plt.show()\n",
    "\n",
    "# print df.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = df.unstack()\n",
    "# df.columns = df.columns.droplevel(0)\n",
    "\n",
    "\n",
    "# # # correlations\n",
    "# # robust_th = 5\n",
    "# # corr_geolevel_yr = np.log10(df).corr(min_periods = robust_th)\n",
    "# # del corr_geolevel_yr.index.name\n",
    "# # del corr_geolevel_yr.columns.name\n",
    "\n",
    "# # df_list += [pd.DataFrame(corr_geolevel_yr.stack(), columns = [col_name])]\n",
    "\n",
    "# # print 'done conf '+ str(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(df.stack()).reset_index().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(corr_geolevel_yr.stack(), columns = [col_name]).reset_index().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNAE_lev = 'S'\n",
    "\n",
    "# x = wages_bra_4y_gl.merge(CBO_agg_ref).groupby(['BRA ID '+Geo_lev, 'CNAE ID C', 'CBO ID '+CBO_lev]).sum()[['Total Yearly Wages']].reset_index().sample(frac = 0.1)\n",
    "# x = x.merge(CNAE_agg_ref).groupby(['BRA ID '+Geo_lev, 'CNAE ID '+CNAE_lev, 'CBO ID '+CBO_lev]).sum()[['Total Yearly Wages']].reset_index()\n",
    "\n",
    "# x.groupby('CNAE ID S').sum()[['Total Yearly Wages']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z = trade_bra_4y_gl.merge(GEO_bra).merge(HS[['HS ID 4', 'HS ID 2']].drop_duplicates()).groupby(['BRA ID '+Geo_lev, 'HS ID '+HS_Exp]).sum().reset_index()\n",
    "# d = z.groupby(['HS ID '+HS_Imp]).sum().sort_values(by = 'Imports')\n",
    "# d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting data == correlations!!! for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demean_df(df):\n",
    "    dm1 = df - df.mean()\n",
    "    dm2 = (dm1.T - dm1.T.mean()).T\n",
    "    \n",
    "    demeaned_df = dm2\n",
    "    \n",
    "    return demeaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "G_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Geo_lev = 'MIR'\n",
    "\n",
    "# trade_bra_4y_gl = trade_bra_4y.merge(GEO_bra).groupby(['BRA ID '+Geo_lev, 'HS ID 4']).sum().reset_index()\n",
    "# wages_bra_4y_gl = wages_bra_4y.merge(GEO_bra).groupby(['BRA ID '+Geo_lev, 'CNAE ID C', 'CBO ID F']).sum().reset_index()\n",
    "\n",
    "for i in range(len(confs)):\n",
    "    conf = confs[i]\n",
    "    print conf\n",
    "\n",
    "    dem_corr = demean_df(df_list[i].unstack())\n",
    "    dem_corr.columns = dem_corr.columns.droplevel(0)\n",
    "\n",
    "    ## standardizing after demeaning\n",
    "    std_dem_corr = dem_corr.div(dem_corr.std(), axis = 0)    \n",
    "\n",
    "    name = 'corr_'+Geo_lev+'_'+'_'.join([c for c in conf])\n",
    "\n",
    "    df = pd.DataFrame(std_dem_corr.stack(), columns = [name])\n",
    "    network_data = df[(df[name] > df[name].quantile(4/5.)) & (df[name] < df[name].quantile(.99))]\n",
    "    network_data.index.names = ['label4_x', 'label_y']\n",
    "\n",
    "    edges = network_data.reset_index().rename({name: 'weight'}, axis  =1)\n",
    "\n",
    "    G = nx.from_pandas_dataframe(edges, 'label4_x', 'label_y', 'weight')\n",
    "    col_name = 'corr_'+Geo_lev+'_'.join([c for c in confs[i]])\n",
    "    nx.write_gexf(G, col_name+'.gexf')\n",
    "\n",
    "    G_list += [G]\n",
    "    # #Main component\n",
    "    # grcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "    # G0=grcc[0]\n",
    "\n",
    "    nodes = nodes_df_list[i].loc[G.nodes()]\n",
    "\n",
    "    #Perform Graph Drawing\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    posi = nx.spring_layout(G, k=0.15)\n",
    "    node_sizes = .001*np.sqrt(nodes['node_value_USD'].values)\n",
    "    colors = nodes.att_A.rank(method = 'dense').values\n",
    "\n",
    "    # \n",
    "\n",
    "    nx.draw(G, posi, node_size = node_sizes, alpha=.6, node_color= colors, edgelist = [])\n",
    "    nx.draw_networkx_edges(G, posi, edge_color='.7', alpha=.3)\n",
    "    #     plt.savefig('../DataViva/Figures/'+'_'+kw+'_'+name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nodes DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 4\n",
    "\n",
    "### Create and save node data\n",
    "\n",
    "nodes = nodes_df_list[j]\n",
    "\n",
    "value = nodes['node_value_USD']\n",
    "# nodes['node_size'] = np.sqrt((value/value.median()).values)\n",
    "nodes['node_size'] = 10 * np.power((value/value.median()).values, 1/4.)\n",
    "\n",
    "nodes['color_code'] = nodes.att_A.rank(method = 'dense')\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "x = plt.cm.get_cmap('rainbow', len(nodes.color_code.unique()))\n",
    "\n",
    "colors_hex = []\n",
    "for i in range(x.N):\n",
    "    rgb = x(i)[:3] # will return rgba, we take only first 3 so we get rgb\n",
    "    colors_hex += [matplotlib.colors.rgb2hex(rgb)]\n",
    "\n",
    "colors_hex = [x.encode('UTF8') for x in colors_hex]\n",
    "\n",
    "\n",
    "code_hex = pd.DataFrame([nodes.color_code.unique(), colors_hex], index = ['color_code', 'color']).T\n",
    "nodes = nodes.reset_index().merge(code_hex)\n",
    "\n",
    "nodes.to_csv(name_list[j]+'_nodes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Load node data\n",
    "# for j in range(len(name_list)):\n",
    "#     nodes = pd.read_csv(name_list[j]+'_nodes.csv').rename({'Unnamed: 0': 'index'}, axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dem_corr = demean_df(df_list[j].unstack())\n",
    "dem_corr.columns = dem_corr.columns.droplevel(0)\n",
    "\n",
    "## standardizing after demeaning\n",
    "std_dem_corr = dem_corr.div(dem_corr.std(), axis = 0)    \n",
    "\n",
    "name = 'corr_'+Geo_lev+'_'+'_'.join([c for c in conf])\n",
    "\n",
    "df = pd.DataFrame(std_dem_corr.stack(), columns = [name])\n",
    "network_data = df[(df[name] > df[name].quantile(4/5.)) & (df[name] < df[name].quantile(.99))]\n",
    "network_data.index.names = ['label_x', 'label_y']\n",
    "\n",
    "edges = network_data.reset_index().rename({name: 'weight'}, axis  =1)\n",
    "\n",
    "# G = nx.from_pandas_dataframe(edges, 'label_x', 'label_y', 'weight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys    \n",
    "# reload(sys)  \n",
    "# sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nodes.sample(30).sort_values(by = 'node_size')\n",
    "# df_list[j].reset_index().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, MultiLine, NodesOnly\n",
    "from bokeh.models.graphs import from_networkx\n",
    "from bokeh.models.tools import HoverTool\n",
    "\n",
    "\n",
    "\n",
    "N = len(nodes)\n",
    "node_indices = list(range(N))\n",
    "# sizes = np.linspace(10, 20, N)\n",
    "# node_sizes = np.sqrt(nodes['node_size'].values)\n",
    "\n",
    "\n",
    "plot = figure(title=\"Networkx Integration Demonstration\", x_range=(-1.1,1.1), y_range=(-1.1,1.1))\n",
    "\n",
    "#####\n",
    "\n",
    "graph = from_networkx(G, nx.spring_layout, center=(0,0), k = .15)\n",
    "\n",
    "graph.node_renderer.data_source.add(list(nodes['color'].values), 'color')\n",
    "graph.node_renderer.data_source.add(list(nodes['node_name'].values), 'node_name')\n",
    "graph.node_renderer.data_source.add(list(nodes['node_size'].values), 'node_size')\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='node_size', fill_color='color')\n",
    "graph.node_renderer.selection_glyph = Circle(size=15, fill_color='#808000')\n",
    "graph.node_renderer.hover_glyph = Circle(size=15, fill_color='#808000')\n",
    "\n",
    "graph.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph.edge_renderer.selection_glyph = MultiLine(line_color='#808000', line_width=5)\n",
    "graph.edge_renderer.hover_glyph = MultiLine(line_color='#808000', line_width=5)\n",
    "graph.selection_policy = NodesOnly()\n",
    "\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "hover = HoverTool(plot=plot, tooltips=dict(act=\"@node_name\", value=\"@node_size\"))\n",
    "# hover = HoverTool(tooltips = [('Info', '@node_name'),('(x,y)', '($x, $y)')])\n",
    "\n",
    "plot.add_tools(hover)\n",
    "\n",
    "output_file(\"networkx_graph.html\")\n",
    "# output_notebook()\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNAE[['CNAE ID S', 'CNAE label S']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CBO[['CBO ID GG', 'CBO label GG']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### PLOT CORR VALUES\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    rk = pd.DataFrame(df_list[i].reset_index().groupby('level_0').mean().rank())\n",
    "    rk.columns = ['rank']\n",
    "    \n",
    "    dem_corr = demean_df(df_list[i].unstack())\n",
    "    dem_corr.columns = dem_corr.columns.droplevel(0)\n",
    "\n",
    "    ## standardizing after demeaning\n",
    "    std_dem_corr = dem_corr.div(dem_corr.std(), axis = 0)   \n",
    "\n",
    "    df_corr = df_list[i].reset_index().merge(rk.reset_index())\n",
    "    df_corr.columns = [u'level_0', u'level_1', 0, u'rank']\n",
    "    df_std_dem = std_dem_corr.stack().reset_index().merge(rk.reset_index())\n",
    "\n",
    "\n",
    "    dfs = [df_corr, df_std_dem]\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (10, 4))\n",
    "    for i in range(2):\n",
    "        df = dfs[i]\n",
    "        ax = axs[i]\n",
    "        df.sample(10000, replace = True).plot(x = 0, y = 'rank', marker = '.', linestyle = 'None', alpha = .1, ax = ax, color = '.5')\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_xlim(-.5, 1)\n",
    "            ax.set_title('Pearson corr values \\n for all nodes, ' +name)\n",
    "            hilite = df_corr.loc[df_std_dem[0] > np.percentile(df_std_dem[0], 100 * 4/5.)]\n",
    "            hilite.sample(3000, replace = True).plot(x = 0, y = 'rank', marker = '.', linestyle = 'None', ax = ax, alpha = .1)\n",
    "\n",
    "\n",
    "        if i ==1:\n",
    "            ax.axvline(x = df[0].quantile(4/5.), linestyle = '-', color = 'b')\n",
    "            ax.axvline(x = df[0].quantile(.99), linestyle = '-', color = 'b')\n",
    "            ax.set_xlim(-4, 4)\n",
    "            ax.set_title('Demeaned and standardized \\n corr values, '+name)\n",
    "#             hilite = df_std_dem.loc[df_std_dem[0] > np.percentile(df_std_dem[0], 100 * 4/5.)]\n",
    "#             hilite.sample(1000, replace = True).plot(x = 0, y = 'rank', marker = '.', linestyle = 'None', ax = ax)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Node size info (maybe should be done when the node labels are set)\n",
    "\n",
    "df = trade_bra.merge(HS[['HS ID 2', 'HS ID 4']].drop_duplicates()).groupby('HS ID 2').sum()\n",
    "nds_y = nodes_df.loc[[row for row in nodes_df.index if 'Exports' in row]].reset_index().merge(df[['Exports']].reset_index(), left_on = 'att_A', right_on = 'HS ID 2').drop('HS ID 2', axis = 1).rename({'Exports' : 'value_USD'}, axis = 1)\n",
    "nds_z = nodes_df.loc[[row for row in nodes_df.index if 'Imports' in row]].reset_index().merge(df[['Imports']].reset_index(), left_on = 'att_A', right_on = 'HS ID 2').drop('HS ID 2', axis = 1).rename({'Imports' : 'value_USD'}, axis = 1)\n",
    "\n",
    "# df = wages_bra.merge(HS['CNAE ID '+CNAE_lev, 'CBO ID '+CBO_lev].drop_duplicates()).groupby(['CNAE ID '+CNAE_lev, 'CBO ID '+CBO_lev]).sum()\n",
    "\n",
    "nds_x = nodes_df.loc[[row for row in nodes_df.index if 'Wages' in row]].reset_index()\n",
    "nds_x['value_USD'] = 1762904178 #shortcut\n",
    "\n",
    "nodes_df = pd.concat([nds_x, nds_y, nds_z]).set_index('node_name')\n",
    "\n",
    "# colors...\n",
    "# nodes_df.merge(CNAE[['CNAE ID D', 'CNAE ID S']], left_on = 'att_A', right_on = 'CNAE ID D', how = 'left').sample(30)#.loc[G.nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.concat(df_list, axis = 1).to_csv('data/Corr/corr_MER_S_GG_2_2', index = False)\n",
    "# pd.concat(df_list, axis = 1).to_csv('data/Corr/corr_MER_S_GG_2_2_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Corr/corr_MER_S_GG_2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "pp = sns.pairplot(df.sample(10000).dropna(), diag_kind=\"kde\", markers=\"+\",\n",
    "             plot_kws=dict(s=50, edgecolor=\"b\", linewidth=1, alpha = .01),\n",
    "             diag_kws=dict(shade=True))\n",
    "[[ax.set_ylim(-.5,1) for ax in ax_list] for ax_list in pp.axes]\n",
    "x = [-.5, 1]\n",
    "[[ax.plot(x,x, c = '.5', linestyle = '--') for ax in ax_list] for ax_list in pp.axes]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
